---
title: "HWCh7.1"
author: "Esteban"
date: "July 4, 2019"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
##7E1. State the three motivating criteria that define information entropy. Try to express each in your own words.

>The measure of uncertainty should:  
1. be continuous  
2. increase as the number of possible events increases.  
3. be additive  

##7E2. Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads 70% of the time. What is the entropy of this coin?
```{r}
p <- c(.7,.3)
-sum( p * log(p) )
```

##7E3. Suppose a four-sided die is loaded such that, when tossed onto a table, it shows “1” 20%, “2” 25%, ”3” 25%, and ”4” 30% of the time. What is the entropy of this die?
```{r}
p <- c(.2, .25, .25, .3)
-sum( p * log(p) )
```

##7E4. Suppose another four-sided die is loaded such that it never shows “4”. The other three sides show equally often. What is the entropy of this die?
```{r}
p <- c(1/3, 1/3, 1/3)
-sum( p * log(p) )
```

